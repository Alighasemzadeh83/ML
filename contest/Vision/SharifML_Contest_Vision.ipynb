{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 dir=rtl align=center style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "    SharifML Contest - Vision\n",
    "</font>\n",
    "</h1>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "   \n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 dir=rtl align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "معرفی مجموعه تصاویر\n",
    "</font>\n",
    "</h2>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "    در این سوال به‌طور کلی سه تا پوشه وجود دارد:\n",
    "    <br>\n",
    "    تصاویر مربوط به تفنگ‌های کوشا در پوشه <code>gun</code> قرار دارند.\n",
    "    <br>\n",
    "    تصاویر مربوط به \"زبل خان\" در پوشه <code>zebel</code> قرار دارند.\n",
    "    <br>\n",
    "    تصاویر مربوط به حیوانات در پوشه <code>animals</code> قرار دارند.\n",
    "    <br>\n",
    "    تصاویر مربوط به مجوعه‌داده آزمون در پوشه <code>test_images</code> قرار دارند.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 dir=rtl align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "ساختن مجموعه تصاویر\n",
    "</font>\n",
    "</h2>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "در ابتدا نیاز است که با استفاده از تصاویری که کوشا در اختیارتان گذاشته، یک مجموعه‌داده با شرایط گفته‌شده تولید کنید.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T15:00:21.847677Z",
     "start_time": "2025-01-26T15:00:21.819393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import gc\n",
    "import random\n",
    "import math\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T17:35:23.081956Z",
     "start_time": "2025-01-26T17:35:23.066324Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_angle(x1, y1, x2, y2):\n",
    "    dx = x2 - x1\n",
    "    dy = y2 - y1\n",
    "    \n",
    "    angle_radians = math.atan2(dy, dx)\n",
    "    angle_degrees = math.degrees(angle_radians)\n",
    "    if angle_degrees > 90:\n",
    "        angle_degrees -= 180 \n",
    "    elif angle_degrees < -90:\n",
    "        angle_degrees += 180\n",
    "    return angle_degrees\n",
    "\n",
    "\n",
    "def calculate_angle_np(x1, y1, x2, y2):\n",
    "    dx = x2 - x1\n",
    "    dy = y2 - y1\n",
    "    angle_radians = np.arctan2(dy, dx)\n",
    "\n",
    "    angle_degrees = np.degrees(angle_radians)\n",
    "\n",
    "    angle_degrees[angle_degrees > 90] -= 180\n",
    "    angle_degrees[angle_degrees < -90] += 180\n",
    "\n",
    "    return angle_degrees"
   ],
   "outputs": [],
   "execution_count": 154
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T17:38:10.540245800Z",
     "start_time": "2025-01-26T17:35:23.961576Z"
    }
   },
   "source": [
    "num_images = 10000\n",
    "result_folder = 'train_images'\n",
    "\n",
    "# num_images = 1000\n",
    "# result_folder = 'val_images'\n",
    "\n",
    "for sample in range(num_images):\n",
    "    canvas = Image.new('RGBA', (500, 300), (255, 255, 255, 255))\n",
    "\n",
    "    image_paths = ['./gun/' + np.random.choice(os.listdir('gun'))] + [\n",
    "        './zebel/' + np.random.choice(os.listdir('zebel'))] + ['./animal/' + np.random.choice(os.listdir('animal')) for\n",
    "                                                               i in range(random.randint(0, 4))]\n",
    "    images = [Image.open(path).convert('RGBA') for path in image_paths]\n",
    "    for i in range(len(images)):\n",
    "        max_edge = max(images[i].width, images[i].height)\n",
    "        scale_factor = random.randint(80, 120) / max_edge\n",
    "\n",
    "        new_size = (\n",
    "            int(images[i].width * scale_factor),\n",
    "            int(images[i].height * scale_factor)\n",
    "        )\n",
    "\n",
    "        images[i] = images[i].resize(new_size, Image.Resampling.LANCZOS)\n",
    "\n",
    "    positions = []\n",
    "    for i in range(len(images)):\n",
    "        x = np.random.randint(250, 500 - images[i].width) if i != 0 else 0\n",
    "        y = np.random.randint(0, 300 - images[i].height)\n",
    "        positions.append((x, y))\n",
    "\n",
    "    for i in np.random.permutation(range(len(images))):\n",
    "        img = images[i]\n",
    "        canvas.paste(img, positions[i], img)\n",
    "\n",
    "    x1 = positions[0][0] + images[0].width / 2\n",
    "    y1 = positions[0][1] + images[0].height / 2\n",
    "    x2 = positions[1][0] + images[1].width / 2\n",
    "    y2 = positions[1][1] + images[1].height / 2\n",
    "    label = calculate_angle(x1, y1, x2, y2)\n",
    "    canvas.save(f'{result_folder}/{sample}_{label}_{x1}_{y1}_{x2}_{y2}_.png')"
   ],
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[155], line 23\u001B[0m\n\u001B[0;32m     16\u001B[0m     scale_factor \u001B[38;5;241m=\u001B[39m random\u001B[38;5;241m.\u001B[39mrandint(\u001B[38;5;241m80\u001B[39m, \u001B[38;5;241m120\u001B[39m) \u001B[38;5;241m/\u001B[39m max_edge\n\u001B[0;32m     18\u001B[0m     new_size \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m     19\u001B[0m         \u001B[38;5;28mint\u001B[39m(images[i]\u001B[38;5;241m.\u001B[39mwidth \u001B[38;5;241m*\u001B[39m scale_factor),\n\u001B[0;32m     20\u001B[0m         \u001B[38;5;28mint\u001B[39m(images[i]\u001B[38;5;241m.\u001B[39mheight \u001B[38;5;241m*\u001B[39m scale_factor)\n\u001B[0;32m     21\u001B[0m     )\n\u001B[1;32m---> 23\u001B[0m     images[i] \u001B[38;5;241m=\u001B[39m \u001B[43mimages\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnew_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mImage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mResampling\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mLANCZOS\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     25\u001B[0m positions \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(images)):\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:2305\u001B[0m, in \u001B[0;36mImage.resize\u001B[1;34m(self, size, resample, box, reducing_gap)\u001B[0m\n\u001B[0;32m   2303\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLA\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRGBA\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;129;01mand\u001B[39;00m resample \u001B[38;5;241m!=\u001B[39m Resampling\u001B[38;5;241m.\u001B[39mNEAREST:\n\u001B[0;32m   2304\u001B[0m     im \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconvert({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLA\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLa\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRGBA\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRGBa\u001B[39m\u001B[38;5;124m\"\u001B[39m}[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode])\n\u001B[1;32m-> 2305\u001B[0m     im \u001B[38;5;241m=\u001B[39m \u001B[43mim\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresize\u001B[49m\u001B[43m(\u001B[49m\u001B[43msize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresample\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbox\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2306\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m im\u001B[38;5;241m.\u001B[39mconvert(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode)\n\u001B[0;32m   2308\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mload()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:2328\u001B[0m, in \u001B[0;36mImage.resize\u001B[1;34m(self, size, resample, box, reducing_gap)\u001B[0m\n\u001B[0;32m   2316\u001B[0m         \u001B[38;5;28mself\u001B[39m \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m   2317\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreduce(factor, box\u001B[38;5;241m=\u001B[39mreduce_box)\n\u001B[0;32m   2318\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcallable\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreduce)\n\u001B[0;32m   2319\u001B[0m             \u001B[38;5;28;01melse\u001B[39;00m Image\u001B[38;5;241m.\u001B[39mreduce(\u001B[38;5;28mself\u001B[39m, factor, box\u001B[38;5;241m=\u001B[39mreduce_box)\n\u001B[0;32m   2320\u001B[0m         )\n\u001B[0;32m   2321\u001B[0m         box \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m   2322\u001B[0m             (box[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m-\u001B[39m reduce_box[\u001B[38;5;241m0\u001B[39m]) \u001B[38;5;241m/\u001B[39m factor_x,\n\u001B[0;32m   2323\u001B[0m             (box[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m-\u001B[39m reduce_box[\u001B[38;5;241m1\u001B[39m]) \u001B[38;5;241m/\u001B[39m factor_y,\n\u001B[0;32m   2324\u001B[0m             (box[\u001B[38;5;241m2\u001B[39m] \u001B[38;5;241m-\u001B[39m reduce_box[\u001B[38;5;241m0\u001B[39m]) \u001B[38;5;241m/\u001B[39m factor_x,\n\u001B[0;32m   2325\u001B[0m             (box[\u001B[38;5;241m3\u001B[39m] \u001B[38;5;241m-\u001B[39m reduce_box[\u001B[38;5;241m1\u001B[39m]) \u001B[38;5;241m/\u001B[39m factor_y,\n\u001B[0;32m   2326\u001B[0m         )\n\u001B[1;32m-> 2328\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_new(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mim\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresize\u001B[49m\u001B[43m(\u001B[49m\u001B[43msize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresample\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbox\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 155
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 dir=rtl align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "آموزش مدل\n",
    "</font>\n",
    "</h2>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "    حال که مجموعه تصاویر را ساخته‌اید، وقت آن است که مدلی آموزش دهید که بتواند متغیر هدف این مسئله را پیش‌بینی کند.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T17:38:33.402324Z",
     "start_time": "2025-01-26T17:38:33.386768Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_dir):\n",
    "        self.image_dir = image_dir\n",
    "        self.image_files = [f for f in os.listdir(image_dir) if f.endswith('.png')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        args = img_name.split('_')\n",
    "        _, label_str, x1_str, y1_str, x2_str, y2_str, _ = args\n",
    "        label, x1, y1, x2, y2 = float(label_str), float(x1_str), float(y1_str), float(x2_str), float(y2_str)\n",
    "\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        image = transform(image)\n",
    "        label = torch.tensor(label)\n",
    "        positions = torch.tensor([x1, y1, x2, y2])\n",
    "\n",
    "        return image, label, positions\n",
    "\n",
    "\n",
    "train_dataset = ImageDataset('train_images')\n",
    "val_dataset = ImageDataset('val_images')\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ],
   "outputs": [],
   "execution_count": 156
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T17:38:36.087175Z",
     "start_time": "2025-01-26T17:38:35.883269Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ],
   "outputs": [],
   "execution_count": 157
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T17:17:20.449253Z",
     "start_time": "2025-01-26T17:17:20.305476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision import models\n",
    "\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.fc = nn.Linear(model.fc.in_features, 1)\n",
    "model = model.to(device)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maabb\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\maabb\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "execution_count": 147
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T17:17:21.423170Z",
     "start_time": "2025-01-26T17:17:21.407547Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0003)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.8)\n",
    "criterion = nn.MSELoss()\n",
    "num_epochs = 30"
   ],
   "outputs": [],
   "execution_count": 148
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T17:32:24.358303Z",
     "start_time": "2025-01-26T17:17:25.512078Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for epoch in range(num_epochs):\n",
    "    epoch_losses = []\n",
    "    model.train()\n",
    "\n",
    "    for image, label, _ in train_dataloader:\n",
    "        image = image.to(device)\n",
    "        label = label.to(device).unsqueeze(-1)\n",
    "\n",
    "        pred = model(image)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = criterion(pred, label)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_losses.append(loss.item())\n",
    "\n",
    "    val_losses = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for image, label, _ in val_dataloader:\n",
    "            image = image.to(device)\n",
    "            label = label.to(device).unsqueeze(-1)\n",
    "\n",
    "            pred = model(image)\n",
    "\n",
    "            loss = criterion(pred, label)\n",
    "\n",
    "            val_losses.append(loss.item())\n",
    "\n",
    "    scheduler.step()\n",
    "    mean_loss = np.mean(epoch_losses)\n",
    "    val_mean_loss = np.mean(val_losses)\n",
    "    print(f\"EPOCH#{epoch:2d},\\t Train_Loss:{mean_loss:8.6f} \\t Val_Loss:{val_mean_loss:8.6f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH# 0,\t Train_Loss:22.101429 \t Val_Loss:1.371396\n",
      "EPOCH# 1,\t Train_Loss:8.070050 \t Val_Loss:2.295840\n",
      "EPOCH# 2,\t Train_Loss:3.343343 \t Val_Loss:0.608417\n",
      "EPOCH# 3,\t Train_Loss:1.444623 \t Val_Loss:2.956528\n",
      "EPOCH# 4,\t Train_Loss:0.827421 \t Val_Loss:0.974513\n",
      "EPOCH# 5,\t Train_Loss:0.534892 \t Val_Loss:0.936114\n",
      "EPOCH# 6,\t Train_Loss:0.497332 \t Val_Loss:0.179758\n",
      "EPOCH# 7,\t Train_Loss:0.421224 \t Val_Loss:0.262524\n",
      "EPOCH# 8,\t Train_Loss:0.429310 \t Val_Loss:0.379329\n",
      "EPOCH# 9,\t Train_Loss:0.309239 \t Val_Loss:0.296010\n",
      "EPOCH#10,\t Train_Loss:0.254135 \t Val_Loss:0.142019\n",
      "EPOCH#11,\t Train_Loss:0.235514 \t Val_Loss:0.374138\n",
      "EPOCH#12,\t Train_Loss:0.200207 \t Val_Loss:0.241589\n",
      "EPOCH#13,\t Train_Loss:0.295264 \t Val_Loss:0.401710\n",
      "EPOCH#14,\t Train_Loss:0.361185 \t Val_Loss:0.401215\n",
      "EPOCH#15,\t Train_Loss:0.148023 \t Val_Loss:0.349817\n",
      "EPOCH#16,\t Train_Loss:0.153478 \t Val_Loss:0.204208\n",
      "EPOCH#17,\t Train_Loss:0.153689 \t Val_Loss:0.143967\n",
      "EPOCH#18,\t Train_Loss:0.161169 \t Val_Loss:0.218714\n",
      "EPOCH#19,\t Train_Loss:0.170175 \t Val_Loss:0.193556\n",
      "EPOCH#20,\t Train_Loss:0.118136 \t Val_Loss:0.244305\n",
      "EPOCH#21,\t Train_Loss:0.103413 \t Val_Loss:0.500004\n",
      "EPOCH#22,\t Train_Loss:0.125988 \t Val_Loss:0.201566\n",
      "EPOCH#23,\t Train_Loss:0.087753 \t Val_Loss:0.101422\n",
      "EPOCH#24,\t Train_Loss:0.163436 \t Val_Loss:0.207417\n",
      "EPOCH#25,\t Train_Loss:0.076316 \t Val_Loss:0.135548\n",
      "EPOCH#26,\t Train_Loss:0.062009 \t Val_Loss:0.195478\n",
      "EPOCH#27,\t Train_Loss:0.058478 \t Val_Loss:0.085267\n",
      "EPOCH#28,\t Train_Loss:0.058645 \t Val_Loss:0.081974\n",
      "EPOCH#29,\t Train_Loss:0.070405 \t Val_Loss:0.129331\n"
     ]
    }
   ],
   "execution_count": 149
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T17:32:27.079784Z",
     "start_time": "2025-01-26T17:32:27.032904Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(model.state_dict(), 'model3.pth')",
   "outputs": [],
   "execution_count": 150
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T17:38:43.854422Z",
     "start_time": "2025-01-26T17:38:43.682077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model2 = models.resnet18(pretrained=True)\n",
    "model2.fc = nn.Linear(model.fc.in_features, 4)\n",
    "model2 = model2.to(device)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maabb\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\maabb\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "execution_count": 158
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T17:38:49.241694Z",
     "start_time": "2025-01-26T17:38:49.226057Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optimizer = torch.optim.Adam(model2.parameters(), lr=0.0003)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.8)\n",
    "criterion = nn.MSELoss()\n",
    "num_epochs = 5"
   ],
   "outputs": [],
   "execution_count": 159
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T17:42:58.794218Z",
     "start_time": "2025-01-26T17:39:07.750691Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for epoch in range(num_epochs):\n",
    "    epoch_losses = []\n",
    "    model2.train()\n",
    "\n",
    "    for image, label, positions in train_dataloader:\n",
    "        image = image.to(device)\n",
    "        positions = positions.to(device)\n",
    "\n",
    "        pred = model2(image)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = criterion(pred, positions / 100)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_losses.append(loss.item())\n",
    "\n",
    "    val_losses = [0]\n",
    "    # model2.eval()\n",
    "    # with torch.no_grad():\n",
    "    #     for image, label, positions in val_dataloader:\n",
    "    #         image = image.to(device)\n",
    "    #         positions = positions.to(device)\n",
    "    # \n",
    "    #         pred = model2(image)\n",
    "    # \n",
    "    #         loss = criterion(pred, positions / 100)\n",
    "    # \n",
    "    #         val_losses.append(loss.item())\n",
    "\n",
    "    scheduler.step()\n",
    "    mean_loss = np.mean(epoch_losses)\n",
    "    val_mean_loss = np.mean(val_losses)\n",
    "    print(f\"EPOCH#{epoch:2d},\\t Train_Loss:{mean_loss:8.6f} \\t Val_Loss:{val_mean_loss:8.6f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH# 0,\t Train_Loss:0.066297 \t Val_Loss:0.000000\n",
      "EPOCH# 1,\t Train_Loss:0.002086 \t Val_Loss:0.000000\n",
      "EPOCH# 2,\t Train_Loss:0.001049 \t Val_Loss:0.000000\n",
      "EPOCH# 3,\t Train_Loss:0.000767 \t Val_Loss:0.000000\n",
      "EPOCH# 4,\t Train_Loss:0.000652 \t Val_Loss:0.000000\n"
     ]
    }
   ],
   "execution_count": 160
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T17:42:58.864087Z",
     "start_time": "2025-01-26T17:42:58.809887Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(model2.state_dict(), 'model5.pth')",
   "outputs": [],
   "execution_count": 161
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 dir=rtl align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "معیار ارزیابی\n",
    "</font>\n",
    "</h2>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "    معیاری که برای ارزیابی عملکرد مدل انتخاب کرده‌ایم، به‌صورت زیر است.\n",
    "    <br>\n",
    "    این معیار، سنجه ارزیابی کیفیت مدل شماست. به عبارت بهتر در سامانه داوری هم از همین معیار برای نمره‌دهی استفاده شده است.\n",
    "    <br>\n",
    "    پیشنهاد می‌شود با توجه به این معیار، عملکرد مدل خود را بر روی مجموعه‌ی آموزش یا اعتبارسنجی ارزیابی کنید.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font color=\"red\"><b color='red'>توجه:</b></font>\n",
    "<font face=\"vazir\" size=3>\n",
    "    برای دریافت نمره از این سوال لازم است تا دقت مدل شما از آستانه‌ی ۰.۴ بیشتر باشد.\n",
    "    در صورتی که دقت مدل شما از ۰.۴ کمتر باشد نمره شما \n",
    "    <b>صفر</b>\n",
    "    خواهد شد و در غیر این صورت با فرمول زیر محاسبه می‌شود:\n",
    "</font>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T17:43:02.873411Z",
     "start_time": "2025-01-26T17:43:02.857814Z"
    }
   },
   "source": [
    "# Evaluate your model\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def custom_accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Custom accuracy function for scikit-learn.\n",
    "    Returns the proportion of predictions where the absolute difference \n",
    "    between the true and predicted values is <= 0.1.\n",
    "    \"\"\"\n",
    "    # Compute the absolute difference\n",
    "    abs_diff = np.abs(y_true - y_pred)\n",
    "\n",
    "    # Check if the difference is <= 0.1\n",
    "    within_threshold = abs_diff <= 0.1\n",
    "\n",
    "    # Calculate the proportion of correct predictions\n",
    "    accuracy = np.mean(within_threshold)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Create a scorer for scikit-learn\n",
    "custom_scorer = make_scorer(custom_accuracy, greater_is_better=True)"
   ],
   "outputs": [],
   "execution_count": 162
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 dir=rtl align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    " پیش‌بینی بر روی داده‌ی تست و خروجی\n",
    "</font>\n",
    "</h2>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "    پیش‌بینی مدل خود بر روی داده‌های آزمون را در یک دیتافریم (<code>dataframe</code>) به فرمت زیر ذخیره کنید.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "    توجه داشته باشید که نام دیتافریم باید <code>submission</code> باشد؛ در غیر این‌صورت، سامانه‌ی داوری قادر به ارزیابی خروجی شما نخواهد بود.\n",
    "    این دیتافریم صرفا شامل ۲ ستون با نام‌های <code>file_name</code> و <code>label</code> است و ۱۰۰۰ سطر دارد.\n",
    "    <br>\n",
    "    به ازای هر سطر موجود در مجموعه‌داده‌ی آزمون، باید یک مقدار پیش‌بینی‌شده داشته باشید که مقدار <code>file_name</code> نام تصویر در پوشه‌ی <code>test_images</code> است و مقدار <code>label</code> پیش‌بینی مدل شما برای تصویر است.\n",
    "    به‌عنوان مثال جدول زیر، ۵ سطر ابتدایی دیتافریم <code>submission</code> را نشان می‌دهد. البته این مقادیر به‌صورت فرضی هستند و در جواب شما، ممکن است متفاوت باشند.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<center>\n",
    "<div align=center \n",
    "style=\"direction: ltr;line-height:200%;font-family:vazir;font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "    \n",
    "||<code>file_name</code>|<code>label</code>|\n",
    "|:----:|:-----:|:-----:|\n",
    "|0|image_0001.png|0|\n",
    "|1|image_0002.png|12|\n",
    "|2|image_0003.png|15|\n",
    "|3|image_0004.png|9|\n",
    "|4|image_0005.png|19|\n",
    "\n",
    "</font>\n",
    "</div>\n",
    "</center>"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T17:43:19.803647Z",
     "start_time": "2025-01-26T17:43:17.098026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TestImageDataset(Dataset):\n",
    "    def __init__(self, image_dir):\n",
    "        self.image_dir = image_dir\n",
    "        self.image_files = [f for f in os.listdir(image_dir) if f.endswith('.png')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        image = transform(image)\n",
    "\n",
    "        return image, img_name\n",
    "\n",
    "\n",
    "# Dataset and DataLoader\n",
    "image_dir = 'test_images'\n",
    "dataset = TestImageDataset(image_dir)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "submission_list = []\n",
    "with torch.no_grad():\n",
    "    for images, file_names in dataloader:\n",
    "        images = images.to(device)\n",
    "        # model\n",
    "        # outputs = model(images)\n",
    "        # predicted_labels1 = outputs.squeeze().cpu().numpy()\n",
    "\n",
    "        # model2\n",
    "        outputs = model2(images)\n",
    "        outputs = outputs.cpu().numpy()\n",
    "        x1, y1, x2, y2 = outputs[:, 0], outputs[:, 1], outputs[:, 2], outputs[:, 3]\n",
    "        predicted_labels2 = calculate_angle_np(x1, y1, x2, y2)\n",
    "\n",
    "        for file_name, label in zip(file_names, predicted_labels2):\n",
    "            submission_list.append({'file_name': file_name, 'degree': label})\n",
    "\n",
    "submission = pd.DataFrame(submission_list)\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "submission.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        file_name     degree\n",
       "0  image_0001.png   6.222661\n",
       "1  image_0002.png  16.967422\n",
       "2  image_0003.png  17.553898\n",
       "3  image_0004.png   9.381893\n",
       "4  image_0005.png  19.575699"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>degree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>image_0001.png</td>\n",
       "      <td>6.222661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>image_0002.png</td>\n",
       "      <td>16.967422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>image_0003.png</td>\n",
       "      <td>17.553898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>image_0004.png</td>\n",
       "      <td>9.381893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>image_0005.png</td>\n",
       "      <td>19.575699</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 163
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 dir=rtl align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "<b>سلول جواب‌ساز</b>\n",
    "</font>\n",
    "</h2>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "    برای ساخته‌شدن فایل <code>result.zip</code> سلول زیر را اجرا کنید. توجه داشته باشید که پیش از اجرای سلول زیر تغییرات اعمال شده در نت‌بوک را ذخیره کرده باشید (<code>ctrl+s</code>) در غیر این صورت، در پایان مسابقه نمره شما به صفر تغییر خواهد کرد.\n",
    "    <br>\n",
    "    همچنین اگر از گوگل کولب برای اجرای این فایل نوت‌بوک استفاده می‌کنید، قبل از ارسال فایل <code>result.zip</code>، آخرین نسخه‌ی نوت‌بوک خود را دانلود کرده و داخل فایل ارسالی قرار دهید.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T17:32:55.567533Z",
     "start_time": "2025-01-26T17:32:55.535807Z"
    }
   },
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "if not os.path.exists(os.path.join(os.getcwd(), 'SharifML_Contest_Vision.ipynb')):\n",
    "    %notebook -e SharifML_Contest_Vision.ipynb\n",
    "\n",
    "\n",
    "def compress(file_names):\n",
    "    print(\"File Paths:\")\n",
    "    print(file_names)\n",
    "    compression = zipfile.ZIP_DEFLATED\n",
    "    with zipfile.ZipFile(\"result.zip\", mode=\"w\") as zf:\n",
    "        for file_name in file_names:\n",
    "            zf.write('./' + file_name, file_name, compress_type=compression)\n",
    "\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "file_names = ['SharifML_Contest_Vision.ipynb', 'submission.csv']\n",
    "compress(file_names)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Paths:\n",
      "['SharifML_Contest_Vision.ipynb', 'submission.csv']\n"
     ]
    }
   ],
   "execution_count": 152
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T17:13:20.852367Z",
     "start_time": "2025-01-26T17:13:20.836739Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
